{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from chainer.datasets import TupleDataset\n",
    "\n",
    "\n",
    "class DatsetParseError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _parse_single(line):\n",
    "    data = line.strip().split()[:48]\n",
    "    rank = int(data[0])\n",
    "    qid = int(data[1][4:])\n",
    "    val = map(lambda x: float(x.split(':')[1]), data[2:])\n",
    "    return qid, rank, val\n",
    "\n",
    "\n",
    "def create_dataset(path, size=-1):\n",
    "    \"\"\"\n",
    "    Create dataset from MQ2007 data.\n",
    "    .. warning:: It will create dataset with label in range [0, 1, 2]\n",
    "        It should be no problem for Permutation Probability Loss\n",
    "        but do not plug in other loss function.\n",
    "    \"\"\"\n",
    "    data = collections.defaultdict(lambda: [[], []])\n",
    "    with open(path, mode='r') as fin:\n",
    "        # Data has one json per line\n",
    "        for i, line in enumerate(fin):\n",
    "            q, r, v = _parse_single(line)\n",
    "            if r not in {0, 1, 2}:\n",
    "                raise DatasetParseError(\n",
    "                    \"L%d: Score must be 0, 1 or 2, but found %d\" %\n",
    "                    (i, r)\n",
    "                )\n",
    "            data[q][0].append(r)\n",
    "            data[q][1].append(v)\n",
    "    vectors = []\n",
    "    scores = []\n",
    "    for d in data.values():\n",
    "        v = np.array(d[1], dtype=np.float32)\n",
    "        s = np.array(d[0], dtype=np.float32)\n",
    "        vectors.append(v)\n",
    "        scores.append(s)\n",
    "    s = max(map(len, scores))\n",
    "    vectors_pad = np.zeros((len(vectors), s, v.shape[-1]), dtype=np.float32)\n",
    "    scores_pad = np.zeros((len(scores), s), dtype=np.float32)\n",
    "    length = np.empty((len(scores)), dtype=np.int32)\n",
    "    for i, (s, v) in enumerate(zip(scores, vectors)):\n",
    "        vectors_pad[i, :len(v), :] = v\n",
    "        scores_pad[i, :len(s)] = s\n",
    "        length[i] = len(v)\n",
    "\n",
    "    if size > 0:\n",
    "        # Sample data AFTER all data has been loaded. This is because\n",
    "        # There might be bias in data ordering.\n",
    "        ind = np.random.permutation(len(vectors))[:size]\n",
    "        return TupleDataset(vectors_pad[ind], scores_pad[ind], length[ind])\n",
    "    else:\n",
    "        return TupleDataset(vectors_pad, scores_pad, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.initializers as I\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ListNet(chainer.Chain):\n",
    "    def __init__(self, input_size, n_units, dropout):\n",
    "        super(ListNet, self).__init__(\n",
    "            l1=L.Linear(input_size, n_units, initialW=I.GlorotUniform()),\n",
    "            l2=L.Linear(n_units, n_units, initialW=I.GlorotUniform()),\n",
    "            l3=L.Linear(n_units, 1, initialW=I.GlorotUniform(),\n",
    "                        nobias=True)\n",
    "        )\n",
    "        self.add_persistent(\"_dropout\", dropout)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        s = list(x.shape)\n",
    "        n_tokens = np.prod(s[:-1])\n",
    "        x = F.reshape(x, (n_tokens, -1))\n",
    "        if self._dropout > 0.:\n",
    "            x = F.dropout(x, self._dropout, train=train)\n",
    "        o_1 = F.relu(self.l1(x))\n",
    "\n",
    "        if self._dropout > 0.:\n",
    "            o_1 = F.dropout(o_1, self._dropout, train=train)\n",
    "        o_2 = F.relu(self.l2(o_1))\n",
    "\n",
    "        if self._dropout > 0.:\n",
    "            o_2 = F.dropout(o_2, self._dropout, train=train)\n",
    "        o_3 = self.l3(o_2)\n",
    "\n",
    "        return F.reshape(o_3, s[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_average_precision(probs, labels, length, th):\n",
    "    \"\"\"\n",
    "    Calculate mean average precision.\n",
    "    Label 1 and 2 are regarded \"correct\" as in the original evaluation\n",
    "    script https://onedrive.live.com/?authkey=%21ACnoZZSZVfHPJd0&cid=8FEADC23D838BDA8&id=8FEADC23D838BDA8%21122&parId=8FEADC23D838BDA8%21107&o=OneUp\n",
    "    Args:\n",
    "        probs (numpy.ndarray): list of lists of probability predictions.\n",
    "            [[0.1, 0.8, 0.3, 0.1, 0.6, 0.6, 0.2], [0., ...]...]\n",
    "        labels (numpy.ndarray): list of lists of ground-truth labels.\n",
    "            Each value must be 0, 1 or 2\n",
    "        order (str): {'descending' or 'accending'}\n",
    "    Return:\n",
    "        float\n",
    "    \"\"\"\n",
    "    num_queries = len(probs)\n",
    "    out = 0.0\n",
    "\n",
    "    for i in xrange(len(probs)):\n",
    "        r = probs[i][:length[i]].argsort()\n",
    "        r = r[::-1]\n",
    "        candidates = labels[i, r]\n",
    "        avg_prec = 0.\n",
    "        precisions = []\n",
    "        num_correct = 0.\n",
    "        for i in xrange(min(th, len(candidates))):\n",
    "            if candidates[i] >= 1:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "\n",
    "        if precisions:\n",
    "            avg_prec = sum(precisions) / len(precisions)\n",
    "\n",
    "            out += avg_prec\n",
    "    return out / float(num_queries)\n",
    "\n",
    "\n",
    "def logsumexp(x, mask, zero_pad, axis):\n",
    "    x_exp = F.where(mask, F.exp(x), zero_pad)\n",
    "    return F.log(F.sum(x_exp, axis=axis))\n",
    "\n",
    "\n",
    "def logsoftmax_no_mask(x, mask, zero_pad, axis):\n",
    "    #x = x - F.broadcast_to(F.max(x, keepdims=True), x.shape)\n",
    "    x_logsumexp = logsumexp(x, mask, zero_pad, axis)\n",
    "\n",
    "    # log_p: (batch size, n)\n",
    "    return x - F.broadcast_to(F.expand_dims(x_logsumexp, 1), x.shape)\n",
    "\n",
    "\n",
    "def logsoftmax(x, mask, zero_pad, axis):\n",
    "    return F.where(mask, logsoftmax_no_mask(x, mask, zero_pad, axis), zero_pad)\n",
    "\n",
    "\n",
    "def softmax(x, mask, zero_pad, axis):\n",
    "    x_explogsoftmax = F.exp(logsoftmax_no_mask(x, mask, zero_pad, axis))\n",
    "    return F.where(mask, x_explogsoftmax, zero_pad)\n",
    "\n",
    "\n",
    "\n",
    "def permutation_probability_loss(x, t, length):\n",
    "    \"\"\"Calculate permutation probability distributions (k=1) and measure the\n",
    "    cross entropy over the two distributions.\n",
    "    Args:\n",
    "        x (Variable): Variable holding a 2d array whose element\n",
    "            indicates unnormalized log probability: the first axis of the\n",
    "            variable represents the number of samples, and the second axis\n",
    "            represents the number data in a query.\n",
    "        t (Variable): Variable holding a 2d float32 vector of ground truth\n",
    "            scores. Must be in same size as x.\n",
    "    Returns:\n",
    "        Variable: A variable holding a scalar array of the loss.\n",
    "    \"\"\"\n",
    "    length = length.reshape(-1, 1)\n",
    "    mask = np.tile(np.arange(x.shape[1]).reshape(1, -1), (x.shape[0],  1)) < length\n",
    "    mask = chainer.Variable(mask)\n",
    "    padding = chainer.Variable(np.zeros(x.shape, dtype=x.dtype))\n",
    "\n",
    "    # log_p: (batch size, n)\n",
    "    log_p_x = logsoftmax(x, mask, padding, axis=1)\n",
    "    # p_t: (batch size, n)\n",
    "    log_p_t = logsoftmax(t, mask, padding, axis=1)\n",
    "\n",
    "    # loss normalized over all instances\n",
    "    loss = F.exp(log_p_t) * log_p_t - F.exp(log_p_t) * log_p_x\n",
    "\n",
    "    return F.sum(loss) / float(x.shape[0])\n",
    "\n",
    "\n",
    "def clip_data(x, l):\n",
    "    return x[:, :max(l)]\n",
    "\n",
    "\n",
    "def _run_batch(model, optimizer, batch, device, train):\n",
    "    assert train == (optimizer is not None)\n",
    "    model.cleargrads()\n",
    "\n",
    "    x, t, l = chainer.dataset.concat_examples(batch, device=device)\n",
    "    x = clip_data(x, l)\n",
    "    t = clip_data(t, l)\n",
    "\n",
    "    y = model(chainer.Variable(x), train=train)\n",
    "    loss = permutation_probability_loss(y, chainer.Variable(t), l)\n",
    "    acc = mean_average_precision(y.data, t, l, 100000)\n",
    "    if optimizer is not None:\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "    return float(loss.data), acc\n",
    "\n",
    "\n",
    "def forward_pred(model, dataset, device=None):\n",
    "    loss = 0.\n",
    "    acc = 0.\n",
    "    iterator = chainer.iterators.SerialIterator(dataset, batch_size=4,\n",
    "                                                repeat=False, shuffle=False)\n",
    "    for batch in iterator:\n",
    "        l, a = _run_batch(model, None, batch, device, False)\n",
    "        loss += l * len(batch)\n",
    "        acc += a * len(batch)\n",
    "    return loss / float(len(dataset)), acc / float(len(dataset))\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_itr, n_epoch, dev=None, device=None,\n",
    "          tmp_dir='tmp.model', lr_decay=0.995):\n",
    "    loss = 0.\n",
    "    acc = 0.\n",
    "    min_loss = float('inf')\n",
    "    min_epoch = 0\n",
    "    report_tmpl = \"[{:>3d}] T/loss={:0.6f} T/acc={:0.6f} D/loss={:0.6f} D/acc={:0.6f} lr={:0.6f}\"\n",
    "    for batch in train_itr:\n",
    "        if train_itr.is_new_epoch:\n",
    "            # this is not executed at first epoch\n",
    "            loss_dev, acc_dev = forward_pred(model, dev, device=device)\n",
    "            loss = loss / len(train_itr.dataset)\n",
    "            acc = acc / len(train_itr.dataset)\n",
    "            logging.info(report_tmpl.format(\n",
    "                train_itr.epoch - 1, loss, acc, loss_dev, acc_dev, optimizer.alpha))\n",
    "            if loss_dev < min_loss:\n",
    "                min_loss = loss_dev\n",
    "                min_epoch = train_itr.epoch - 1\n",
    "                chainer.serializers.save_npz(tmp_dir, model)\n",
    "\n",
    "            loss = 0.\n",
    "            acc = 0.\n",
    "            optimizer.alpha *= lr_decay\n",
    "        if train_itr.epoch == n_epoch:\n",
    "            break\n",
    "        l, a = _run_batch(model, optimizer, batch, device, True)\n",
    "        loss += l * len(batch)\n",
    "        acc += a * len(batch)\n",
    "\n",
    "    logging.info('loading early stopped-model at epoch {}'.format(min_epoch))\n",
    "    chainer.serializers.load_npz(tmp_dir, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataset\n",
      "INFO:root:loaded 471 sets for training\n",
      "INFO:root:loaded 157 sets for dev\n",
      "INFO:root:loaded 156 sets for test\n",
      "INFO:root:[  0] T/loss=0.142821 T/acc=0.451238 D/loss=0.148315 D/acc=0.487140 lr=0.000700\n",
      "INFO:root:[  1] T/loss=0.137200 T/acc=0.468169 D/loss=0.147032 D/acc=0.496971 lr=0.000696\n",
      "INFO:root:[  2] T/loss=0.136525 T/acc=0.475992 D/loss=0.146266 D/acc=0.506027 lr=0.000693\n",
      "INFO:root:[  3] T/loss=0.135640 T/acc=0.477695 D/loss=0.147637 D/acc=0.515359 lr=0.000690\n",
      "INFO:root:[  4] T/loss=0.135373 T/acc=0.482647 D/loss=0.148527 D/acc=0.522586 lr=0.000686\n",
      "INFO:root:[  5] T/loss=0.134315 T/acc=0.485358 D/loss=0.143353 D/acc=0.516742 lr=0.000683\n",
      "INFO:root:[  6] T/loss=0.132754 T/acc=0.480707 D/loss=0.149556 D/acc=0.501629 lr=0.000679\n",
      "INFO:root:[  7] T/loss=0.132707 T/acc=0.484308 D/loss=0.146763 D/acc=0.493562 lr=0.000676\n",
      "INFO:root:[  8] T/loss=0.133133 T/acc=0.483089 D/loss=0.147581 D/acc=0.508903 lr=0.000672\n",
      "INFO:root:[  9] T/loss=0.132194 T/acc=0.496119 D/loss=0.149219 D/acc=0.512974 lr=0.000669\n",
      "INFO:root:[ 10] T/loss=0.131840 T/acc=0.484854 D/loss=0.145007 D/acc=0.515641 lr=0.000666\n",
      "INFO:root:[ 11] T/loss=0.130412 T/acc=0.482546 D/loss=0.145197 D/acc=0.516779 lr=0.000662\n",
      "INFO:root:[ 12] T/loss=0.130120 T/acc=0.493569 D/loss=0.149141 D/acc=0.487824 lr=0.000659\n",
      "INFO:root:[ 13] T/loss=0.128984 T/acc=0.491016 D/loss=0.147632 D/acc=0.509254 lr=0.000656\n",
      "INFO:root:[ 14] T/loss=0.127938 T/acc=0.496348 D/loss=0.153018 D/acc=0.495097 lr=0.000653\n",
      "INFO:root:[ 15] T/loss=0.128657 T/acc=0.497189 D/loss=0.153891 D/acc=0.512954 lr=0.000649\n",
      "INFO:root:[ 16] T/loss=0.126758 T/acc=0.492667 D/loss=0.153465 D/acc=0.487291 lr=0.000646\n",
      "INFO:root:[ 17] T/loss=0.126986 T/acc=0.494533 D/loss=0.149780 D/acc=0.501668 lr=0.000643\n",
      "INFO:root:[ 18] T/loss=0.128086 T/acc=0.501695 D/loss=0.147584 D/acc=0.510836 lr=0.000640\n",
      "INFO:root:[ 19] T/loss=0.125542 T/acc=0.492811 D/loss=0.150435 D/acc=0.501242 lr=0.000636\n",
      "INFO:root:[ 20] T/loss=0.125497 T/acc=0.497087 D/loss=0.151781 D/acc=0.510658 lr=0.000633\n",
      "INFO:root:[ 21] T/loss=0.125004 T/acc=0.503915 D/loss=0.152182 D/acc=0.504523 lr=0.000630\n",
      "INFO:root:[ 22] T/loss=0.123756 T/acc=0.504938 D/loss=0.148343 D/acc=0.498576 lr=0.000627\n",
      "INFO:root:[ 23] T/loss=0.123647 T/acc=0.502738 D/loss=0.146208 D/acc=0.510448 lr=0.000624\n",
      "INFO:root:[ 24] T/loss=0.122756 T/acc=0.505678 D/loss=0.152791 D/acc=0.493319 lr=0.000621\n",
      "INFO:root:[ 25] T/loss=0.122737 T/acc=0.501166 D/loss=0.150823 D/acc=0.502078 lr=0.000618\n",
      "INFO:root:[ 26] T/loss=0.121759 T/acc=0.512659 D/loss=0.151895 D/acc=0.500165 lr=0.000614\n",
      "INFO:root:[ 27] T/loss=0.121403 T/acc=0.502772 D/loss=0.153618 D/acc=0.489144 lr=0.000611\n",
      "INFO:root:[ 28] T/loss=0.120426 T/acc=0.513349 D/loss=0.151285 D/acc=0.501996 lr=0.000608\n",
      "INFO:root:[ 29] T/loss=0.118362 T/acc=0.507774 D/loss=0.151392 D/acc=0.507934 lr=0.000605\n",
      "INFO:root:[ 30] T/loss=0.120486 T/acc=0.519071 D/loss=0.152343 D/acc=0.497378 lr=0.000602\n",
      "INFO:root:[ 31] T/loss=0.118439 T/acc=0.506567 D/loss=0.150647 D/acc=0.499894 lr=0.000599\n",
      "INFO:root:[ 32] T/loss=0.118032 T/acc=0.512076 D/loss=0.152547 D/acc=0.505828 lr=0.000596\n",
      "INFO:root:[ 33] T/loss=0.118021 T/acc=0.509645 D/loss=0.153925 D/acc=0.494050 lr=0.000593\n",
      "INFO:root:[ 34] T/loss=0.116726 T/acc=0.516398 D/loss=0.152032 D/acc=0.516421 lr=0.000590\n",
      "INFO:root:[ 35] T/loss=0.115335 T/acc=0.514160 D/loss=0.151477 D/acc=0.497747 lr=0.000587\n",
      "INFO:root:[ 36] T/loss=0.116813 T/acc=0.513728 D/loss=0.154539 D/acc=0.498575 lr=0.000584\n",
      "INFO:root:[ 37] T/loss=0.114741 T/acc=0.516926 D/loss=0.155688 D/acc=0.483216 lr=0.000582\n",
      "INFO:root:[ 38] T/loss=0.115653 T/acc=0.519683 D/loss=0.155399 D/acc=0.507905 lr=0.000579\n",
      "INFO:root:[ 39] T/loss=0.113499 T/acc=0.518484 D/loss=0.151109 D/acc=0.501436 lr=0.000576\n",
      "INFO:root:[ 40] T/loss=0.113605 T/acc=0.521380 D/loss=0.150898 D/acc=0.507055 lr=0.000573\n",
      "INFO:root:[ 41] T/loss=0.113723 T/acc=0.520543 D/loss=0.154228 D/acc=0.491818 lr=0.000570\n",
      "INFO:root:[ 42] T/loss=0.112662 T/acc=0.520243 D/loss=0.157101 D/acc=0.512102 lr=0.000567\n",
      "INFO:root:[ 43] T/loss=0.112366 T/acc=0.520321 D/loss=0.154380 D/acc=0.504730 lr=0.000564\n",
      "INFO:root:[ 44] T/loss=0.111557 T/acc=0.525932 D/loss=0.154830 D/acc=0.505271 lr=0.000561\n",
      "INFO:root:[ 45] T/loss=0.111054 T/acc=0.529218 D/loss=0.156540 D/acc=0.503064 lr=0.000559\n",
      "INFO:root:[ 46] T/loss=0.109355 T/acc=0.531075 D/loss=0.152555 D/acc=0.522096 lr=0.000556\n",
      "INFO:root:[ 47] T/loss=0.109394 T/acc=0.527829 D/loss=0.152276 D/acc=0.516194 lr=0.000553\n",
      "INFO:root:[ 48] T/loss=0.109403 T/acc=0.531689 D/loss=0.153987 D/acc=0.506987 lr=0.000550\n",
      "INFO:root:[ 49] T/loss=0.109053 T/acc=0.527821 D/loss=0.157458 D/acc=0.510513 lr=0.000548\n",
      "INFO:root:[ 50] T/loss=0.108028 T/acc=0.529679 D/loss=0.157332 D/acc=0.503819 lr=0.000545\n",
      "INFO:root:[ 51] T/loss=0.107758 T/acc=0.530369 D/loss=0.153748 D/acc=0.505709 lr=0.000542\n",
      "INFO:root:[ 52] T/loss=0.106358 T/acc=0.527823 D/loss=0.155008 D/acc=0.514601 lr=0.000539\n",
      "INFO:root:[ 53] T/loss=0.106348 T/acc=0.530763 D/loss=0.158438 D/acc=0.508480 lr=0.000537\n",
      "INFO:root:[ 54] T/loss=0.106192 T/acc=0.532542 D/loss=0.156046 D/acc=0.498269 lr=0.000534\n",
      "INFO:root:[ 55] T/loss=0.105805 T/acc=0.534529 D/loss=0.159679 D/acc=0.491811 lr=0.000531\n",
      "INFO:root:[ 56] T/loss=0.105389 T/acc=0.537943 D/loss=0.156922 D/acc=0.501665 lr=0.000529\n",
      "INFO:root:[ 57] T/loss=0.106156 T/acc=0.537966 D/loss=0.156654 D/acc=0.505176 lr=0.000526\n",
      "INFO:root:[ 58] T/loss=0.104592 T/acc=0.543701 D/loss=0.155481 D/acc=0.508843 lr=0.000523\n",
      "INFO:root:[ 59] T/loss=0.104513 T/acc=0.545618 D/loss=0.162906 D/acc=0.505036 lr=0.000521\n",
      "INFO:root:[ 60] T/loss=0.103158 T/acc=0.541129 D/loss=0.161295 D/acc=0.495532 lr=0.000518\n",
      "INFO:root:[ 61] T/loss=0.102729 T/acc=0.545073 D/loss=0.160362 D/acc=0.510884 lr=0.000516\n",
      "INFO:root:[ 62] T/loss=0.101295 T/acc=0.537103 D/loss=0.159311 D/acc=0.497788 lr=0.000513\n",
      "INFO:root:[ 63] T/loss=0.102219 T/acc=0.540274 D/loss=0.158363 D/acc=0.484773 lr=0.000510\n",
      "INFO:root:[ 64] T/loss=0.101909 T/acc=0.543199 D/loss=0.160553 D/acc=0.498992 lr=0.000508\n",
      "INFO:root:[ 65] T/loss=0.101234 T/acc=0.545712 D/loss=0.160565 D/acc=0.498796 lr=0.000505\n",
      "INFO:root:[ 66] T/loss=0.101017 T/acc=0.545605 D/loss=0.155809 D/acc=0.496092 lr=0.000503\n",
      "INFO:root:[ 67] T/loss=0.099446 T/acc=0.538771 D/loss=0.162472 D/acc=0.498512 lr=0.000500\n",
      "INFO:root:[ 68] T/loss=0.098431 T/acc=0.545392 D/loss=0.159919 D/acc=0.499203 lr=0.000498\n",
      "INFO:root:[ 69] T/loss=0.099309 T/acc=0.549262 D/loss=0.160055 D/acc=0.494169 lr=0.000495\n",
      "INFO:root:[ 70] T/loss=0.099077 T/acc=0.543320 D/loss=0.171703 D/acc=0.509143 lr=0.000493\n",
      "INFO:root:[ 71] T/loss=0.098035 T/acc=0.544690 D/loss=0.163281 D/acc=0.487754 lr=0.000490\n",
      "INFO:root:[ 72] T/loss=0.097759 T/acc=0.543016 D/loss=0.161281 D/acc=0.477895 lr=0.000488\n",
      "INFO:root:[ 73] T/loss=0.097119 T/acc=0.544188 D/loss=0.165587 D/acc=0.495592 lr=0.000485\n",
      "INFO:root:[ 74] T/loss=0.098468 T/acc=0.547919 D/loss=0.160719 D/acc=0.495698 lr=0.000483\n",
      "INFO:root:[ 75] T/loss=0.096653 T/acc=0.548086 D/loss=0.159910 D/acc=0.504116 lr=0.000481\n",
      "INFO:root:[ 76] T/loss=0.095566 T/acc=0.547634 D/loss=0.159674 D/acc=0.489281 lr=0.000478\n",
      "INFO:root:[ 77] T/loss=0.095367 T/acc=0.552273 D/loss=0.164347 D/acc=0.467583 lr=0.000476\n",
      "INFO:root:[ 78] T/loss=0.095936 T/acc=0.550098 D/loss=0.168630 D/acc=0.502114 lr=0.000473\n",
      "INFO:root:[ 79] T/loss=0.094664 T/acc=0.555724 D/loss=0.163337 D/acc=0.492174 lr=0.000471\n",
      "INFO:root:[ 80] T/loss=0.094712 T/acc=0.550080 D/loss=0.167551 D/acc=0.492795 lr=0.000469\n",
      "INFO:root:[ 81] T/loss=0.094214 T/acc=0.553674 D/loss=0.169739 D/acc=0.497243 lr=0.000466\n",
      "INFO:root:[ 82] T/loss=0.093132 T/acc=0.554808 D/loss=0.164097 D/acc=0.497564 lr=0.000464\n",
      "INFO:root:[ 83] T/loss=0.093919 T/acc=0.547520 D/loss=0.163954 D/acc=0.481420 lr=0.000462\n",
      "INFO:root:[ 84] T/loss=0.093101 T/acc=0.557327 D/loss=0.163564 D/acc=0.483960 lr=0.000459\n",
      "INFO:root:[ 85] T/loss=0.092515 T/acc=0.551282 D/loss=0.164983 D/acc=0.508425 lr=0.000457\n",
      "INFO:root:[ 86] T/loss=0.092301 T/acc=0.550351 D/loss=0.162576 D/acc=0.484436 lr=0.000455\n",
      "INFO:root:[ 87] T/loss=0.092313 T/acc=0.558247 D/loss=0.169790 D/acc=0.487392 lr=0.000453\n",
      "INFO:root:[ 88] T/loss=0.090731 T/acc=0.555706 D/loss=0.170470 D/acc=0.486658 lr=0.000450\n",
      "INFO:root:[ 89] T/loss=0.091121 T/acc=0.564895 D/loss=0.164781 D/acc=0.499104 lr=0.000448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[ 90] T/loss=0.090060 T/acc=0.558016 D/loss=0.173404 D/acc=0.484554 lr=0.000446\n",
      "INFO:root:[ 91] T/loss=0.090318 T/acc=0.552522 D/loss=0.163917 D/acc=0.483026 lr=0.000444\n",
      "INFO:root:[ 92] T/loss=0.089397 T/acc=0.554545 D/loss=0.164941 D/acc=0.494598 lr=0.000441\n",
      "INFO:root:[ 93] T/loss=0.089295 T/acc=0.558467 D/loss=0.164723 D/acc=0.479170 lr=0.000439\n",
      "INFO:root:[ 94] T/loss=0.089214 T/acc=0.558295 D/loss=0.167384 D/acc=0.484254 lr=0.000437\n",
      "INFO:root:[ 95] T/loss=0.087805 T/acc=0.567494 D/loss=0.163780 D/acc=0.492810 lr=0.000435\n",
      "INFO:root:[ 96] T/loss=0.088584 T/acc=0.561060 D/loss=0.178974 D/acc=0.493628 lr=0.000433\n",
      "INFO:root:[ 97] T/loss=0.087695 T/acc=0.557601 D/loss=0.173326 D/acc=0.499136 lr=0.000430\n",
      "INFO:root:[ 98] T/loss=0.087851 T/acc=0.560427 D/loss=0.167595 D/acc=0.489561 lr=0.000428\n",
      "INFO:root:[ 99] T/loss=0.088234 T/acc=0.560517 D/loss=0.161404 D/acc=0.509424 lr=0.000426\n",
      "INFO:root:[100] T/loss=0.086817 T/acc=0.565704 D/loss=0.163929 D/acc=0.482426 lr=0.000424\n",
      "INFO:root:[101] T/loss=0.086494 T/acc=0.562927 D/loss=0.162219 D/acc=0.489533 lr=0.000422\n",
      "INFO:root:[102] T/loss=0.086008 T/acc=0.567626 D/loss=0.167690 D/acc=0.494989 lr=0.000420\n",
      "INFO:root:[103] T/loss=0.085536 T/acc=0.559162 D/loss=0.164787 D/acc=0.479919 lr=0.000418\n",
      "INFO:root:[104] T/loss=0.085931 T/acc=0.566845 D/loss=0.169563 D/acc=0.470817 lr=0.000416\n",
      "INFO:root:[105] T/loss=0.085779 T/acc=0.561610 D/loss=0.166976 D/acc=0.480591 lr=0.000414\n",
      "INFO:root:[106] T/loss=0.084554 T/acc=0.568238 D/loss=0.167287 D/acc=0.483139 lr=0.000411\n",
      "INFO:root:[107] T/loss=0.083771 T/acc=0.566290 D/loss=0.172914 D/acc=0.470096 lr=0.000409\n",
      "INFO:root:[108] T/loss=0.084234 T/acc=0.563445 D/loss=0.166245 D/acc=0.497659 lr=0.000407\n",
      "INFO:root:[109] T/loss=0.083770 T/acc=0.565891 D/loss=0.171405 D/acc=0.490384 lr=0.000405\n",
      "INFO:root:[110] T/loss=0.084005 T/acc=0.568700 D/loss=0.165232 D/acc=0.502349 lr=0.000403\n",
      "INFO:root:[111] T/loss=0.084712 T/acc=0.567015 D/loss=0.165370 D/acc=0.485942 lr=0.000401\n",
      "INFO:root:[112] T/loss=0.083965 T/acc=0.566571 D/loss=0.164699 D/acc=0.469570 lr=0.000399\n",
      "INFO:root:[113] T/loss=0.083734 T/acc=0.567649 D/loss=0.172480 D/acc=0.478382 lr=0.000397\n",
      "INFO:root:[114] T/loss=0.081821 T/acc=0.572188 D/loss=0.173934 D/acc=0.485951 lr=0.000395\n",
      "INFO:root:[115] T/loss=0.082577 T/acc=0.570248 D/loss=0.171070 D/acc=0.483427 lr=0.000393\n",
      "INFO:root:[116] T/loss=0.081645 T/acc=0.570286 D/loss=0.176327 D/acc=0.473166 lr=0.000391\n",
      "INFO:root:[117] T/loss=0.081995 T/acc=0.572890 D/loss=0.171441 D/acc=0.477030 lr=0.000389\n",
      "INFO:root:[118] T/loss=0.082081 T/acc=0.570454 D/loss=0.170520 D/acc=0.490850 lr=0.000387\n",
      "INFO:root:[119] T/loss=0.081716 T/acc=0.568624 D/loss=0.170481 D/acc=0.476664 lr=0.000386\n",
      "INFO:root:[120] T/loss=0.081185 T/acc=0.573619 D/loss=0.176229 D/acc=0.481335 lr=0.000384\n",
      "INFO:root:[121] T/loss=0.081676 T/acc=0.572941 D/loss=0.183769 D/acc=0.474909 lr=0.000382\n",
      "INFO:root:[122] T/loss=0.081282 T/acc=0.571299 D/loss=0.170046 D/acc=0.482826 lr=0.000380\n",
      "INFO:root:[123] T/loss=0.080366 T/acc=0.571095 D/loss=0.172899 D/acc=0.481879 lr=0.000378\n",
      "INFO:root:[124] T/loss=0.080570 T/acc=0.565533 D/loss=0.170442 D/acc=0.478974 lr=0.000376\n",
      "INFO:root:[125] T/loss=0.081285 T/acc=0.572180 D/loss=0.172429 D/acc=0.475668 lr=0.000374\n",
      "INFO:root:[126] T/loss=0.080256 T/acc=0.577659 D/loss=0.172669 D/acc=0.466574 lr=0.000372\n",
      "INFO:root:[127] T/loss=0.078887 T/acc=0.579985 D/loss=0.176993 D/acc=0.487626 lr=0.000370\n",
      "INFO:root:[128] T/loss=0.080485 T/acc=0.571159 D/loss=0.178941 D/acc=0.484507 lr=0.000369\n",
      "INFO:root:[129] T/loss=0.078600 T/acc=0.574877 D/loss=0.174417 D/acc=0.485440 lr=0.000367\n",
      "INFO:root:[130] T/loss=0.079461 T/acc=0.572768 D/loss=0.180240 D/acc=0.478201 lr=0.000365\n",
      "INFO:root:[131] T/loss=0.079053 T/acc=0.578296 D/loss=0.175538 D/acc=0.483344 lr=0.000363\n",
      "INFO:root:[132] T/loss=0.078823 T/acc=0.576661 D/loss=0.171383 D/acc=0.474854 lr=0.000361\n",
      "INFO:root:[133] T/loss=0.077972 T/acc=0.577794 D/loss=0.172876 D/acc=0.479765 lr=0.000359\n",
      "INFO:root:[134] T/loss=0.077629 T/acc=0.580383 D/loss=0.171874 D/acc=0.488704 lr=0.000358\n",
      "INFO:root:[135] T/loss=0.078974 T/acc=0.580622 D/loss=0.172404 D/acc=0.471590 lr=0.000356\n",
      "INFO:root:[136] T/loss=0.077109 T/acc=0.569968 D/loss=0.170272 D/acc=0.479747 lr=0.000354\n",
      "INFO:root:[137] T/loss=0.078194 T/acc=0.575201 D/loss=0.175966 D/acc=0.474830 lr=0.000352\n",
      "INFO:root:[138] T/loss=0.078149 T/acc=0.577271 D/loss=0.172392 D/acc=0.477081 lr=0.000350\n",
      "INFO:root:[139] T/loss=0.076676 T/acc=0.581160 D/loss=0.178531 D/acc=0.472452 lr=0.000349\n",
      "INFO:root:[140] T/loss=0.077978 T/acc=0.574370 D/loss=0.176491 D/acc=0.477981 lr=0.000347\n",
      "INFO:root:[141] T/loss=0.077320 T/acc=0.578061 D/loss=0.178272 D/acc=0.483745 lr=0.000345\n",
      "INFO:root:[142] T/loss=0.076680 T/acc=0.576103 D/loss=0.172403 D/acc=0.488625 lr=0.000344\n",
      "INFO:root:[143] T/loss=0.077589 T/acc=0.574114 D/loss=0.178060 D/acc=0.483024 lr=0.000342\n",
      "INFO:root:[144] T/loss=0.076179 T/acc=0.580104 D/loss=0.178196 D/acc=0.479915 lr=0.000340\n",
      "INFO:root:[145] T/loss=0.076957 T/acc=0.573802 D/loss=0.168873 D/acc=0.475966 lr=0.000338\n",
      "INFO:root:[146] T/loss=0.076518 T/acc=0.577397 D/loss=0.177727 D/acc=0.488314 lr=0.000337\n",
      "INFO:root:[147] T/loss=0.076340 T/acc=0.577716 D/loss=0.181668 D/acc=0.473977 lr=0.000335\n",
      "INFO:root:[148] T/loss=0.075923 T/acc=0.576847 D/loss=0.176514 D/acc=0.484614 lr=0.000333\n",
      "INFO:root:[149] T/loss=0.076492 T/acc=0.577788 D/loss=0.172407 D/acc=0.476473 lr=0.000332\n",
      "INFO:root:[150] T/loss=0.074542 T/acc=0.582995 D/loss=0.180530 D/acc=0.481707 lr=0.000330\n",
      "INFO:root:[151] T/loss=0.076054 T/acc=0.579528 D/loss=0.173519 D/acc=0.476494 lr=0.000328\n",
      "INFO:root:[152] T/loss=0.074489 T/acc=0.580864 D/loss=0.179577 D/acc=0.477422 lr=0.000327\n",
      "INFO:root:[153] T/loss=0.075155 T/acc=0.579873 D/loss=0.186600 D/acc=0.474935 lr=0.000325\n",
      "INFO:root:[154] T/loss=0.075320 T/acc=0.582290 D/loss=0.178574 D/acc=0.487695 lr=0.000323\n",
      "INFO:root:[155] T/loss=0.074739 T/acc=0.582582 D/loss=0.175963 D/acc=0.480473 lr=0.000322\n",
      "INFO:root:[156] T/loss=0.075000 T/acc=0.577644 D/loss=0.176840 D/acc=0.478317 lr=0.000320\n",
      "INFO:root:[157] T/loss=0.074525 T/acc=0.577561 D/loss=0.177829 D/acc=0.481994 lr=0.000319\n",
      "INFO:root:[158] T/loss=0.074023 T/acc=0.581232 D/loss=0.173560 D/acc=0.482039 lr=0.000317\n",
      "INFO:root:[159] T/loss=0.074175 T/acc=0.577247 D/loss=0.185643 D/acc=0.477737 lr=0.000315\n",
      "INFO:root:[160] T/loss=0.073766 T/acc=0.577642 D/loss=0.183092 D/acc=0.471663 lr=0.000314\n",
      "INFO:root:[161] T/loss=0.073852 T/acc=0.579689 D/loss=0.176553 D/acc=0.471707 lr=0.000312\n",
      "INFO:root:[162] T/loss=0.074075 T/acc=0.582791 D/loss=0.182886 D/acc=0.471659 lr=0.000311\n",
      "INFO:root:[163] T/loss=0.073571 T/acc=0.585434 D/loss=0.185803 D/acc=0.474044 lr=0.000309\n",
      "INFO:root:[164] T/loss=0.074007 T/acc=0.577216 D/loss=0.172185 D/acc=0.486617 lr=0.000308\n",
      "INFO:root:[165] T/loss=0.073899 T/acc=0.582452 D/loss=0.175074 D/acc=0.487160 lr=0.000306\n",
      "INFO:root:[166] T/loss=0.072335 T/acc=0.581427 D/loss=0.180055 D/acc=0.494704 lr=0.000305\n",
      "INFO:root:[167] T/loss=0.074021 T/acc=0.582142 D/loss=0.178904 D/acc=0.487681 lr=0.000303\n",
      "INFO:root:[168] T/loss=0.072760 T/acc=0.583250 D/loss=0.174869 D/acc=0.481085 lr=0.000302\n",
      "INFO:root:[169] T/loss=0.073227 T/acc=0.582297 D/loss=0.175672 D/acc=0.480501 lr=0.000300\n",
      "INFO:root:[170] T/loss=0.072786 T/acc=0.583714 D/loss=0.180953 D/acc=0.488184 lr=0.000299\n",
      "INFO:root:[171] T/loss=0.072831 T/acc=0.586324 D/loss=0.182463 D/acc=0.476164 lr=0.000297\n",
      "INFO:root:[172] T/loss=0.072222 T/acc=0.586276 D/loss=0.189856 D/acc=0.474849 lr=0.000296\n",
      "INFO:root:[173] T/loss=0.072434 T/acc=0.581138 D/loss=0.182331 D/acc=0.469093 lr=0.000294\n",
      "INFO:root:[174] T/loss=0.072233 T/acc=0.585503 D/loss=0.181601 D/acc=0.482865 lr=0.000293\n",
      "INFO:root:[175] T/loss=0.072067 T/acc=0.585565 D/loss=0.178001 D/acc=0.476531 lr=0.000291\n",
      "INFO:root:[176] T/loss=0.071007 T/acc=0.584274 D/loss=0.185673 D/acc=0.480440 lr=0.000290\n",
      "INFO:root:[177] T/loss=0.072299 T/acc=0.585094 D/loss=0.181509 D/acc=0.478433 lr=0.000288\n",
      "INFO:root:[178] T/loss=0.071592 T/acc=0.580149 D/loss=0.179712 D/acc=0.486000 lr=0.000287\n",
      "INFO:root:[179] T/loss=0.071734 T/acc=0.585850 D/loss=0.181335 D/acc=0.479441 lr=0.000285\n",
      "INFO:root:[180] T/loss=0.071297 T/acc=0.582974 D/loss=0.185398 D/acc=0.482214 lr=0.000284\n",
      "INFO:root:[181] T/loss=0.071432 T/acc=0.587015 D/loss=0.190026 D/acc=0.471681 lr=0.000283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[182] T/loss=0.070842 T/acc=0.585344 D/loss=0.186485 D/acc=0.474538 lr=0.000281\n",
      "INFO:root:[183] T/loss=0.071069 T/acc=0.587732 D/loss=0.179803 D/acc=0.474761 lr=0.000280\n",
      "INFO:root:[184] T/loss=0.071089 T/acc=0.587563 D/loss=0.179676 D/acc=0.471400 lr=0.000278\n",
      "INFO:root:[185] T/loss=0.070681 T/acc=0.582908 D/loss=0.184562 D/acc=0.478597 lr=0.000277\n",
      "INFO:root:[186] T/loss=0.070313 T/acc=0.586684 D/loss=0.185755 D/acc=0.476697 lr=0.000276\n",
      "INFO:root:[187] T/loss=0.071008 T/acc=0.587157 D/loss=0.181622 D/acc=0.479383 lr=0.000274\n",
      "INFO:root:[188] T/loss=0.070517 T/acc=0.587578 D/loss=0.183738 D/acc=0.481029 lr=0.000273\n",
      "INFO:root:[189] T/loss=0.070484 T/acc=0.585657 D/loss=0.186160 D/acc=0.477504 lr=0.000271\n",
      "INFO:root:[190] T/loss=0.070215 T/acc=0.589141 D/loss=0.180322 D/acc=0.484693 lr=0.000270\n",
      "INFO:root:[191] T/loss=0.070575 T/acc=0.588895 D/loss=0.186704 D/acc=0.475733 lr=0.000269\n",
      "INFO:root:[192] T/loss=0.069568 T/acc=0.585129 D/loss=0.177348 D/acc=0.475296 lr=0.000267\n",
      "INFO:root:[193] T/loss=0.069252 T/acc=0.589889 D/loss=0.182033 D/acc=0.464199 lr=0.000266\n",
      "INFO:root:[194] T/loss=0.069413 T/acc=0.587803 D/loss=0.178224 D/acc=0.476143 lr=0.000265\n",
      "INFO:root:[195] T/loss=0.069023 T/acc=0.588489 D/loss=0.181469 D/acc=0.472491 lr=0.000263\n",
      "INFO:root:[196] T/loss=0.070696 T/acc=0.585982 D/loss=0.183365 D/acc=0.467008 lr=0.000262\n",
      "INFO:root:[197] T/loss=0.069927 T/acc=0.591641 D/loss=0.181966 D/acc=0.475986 lr=0.000261\n",
      "INFO:root:[198] T/loss=0.069433 T/acc=0.585551 D/loss=0.186673 D/acc=0.476292 lr=0.000259\n",
      "INFO:root:[199] T/loss=0.069339 T/acc=0.589381 D/loss=0.184770 D/acc=0.479771 lr=0.000258\n",
      "INFO:root:[200] T/loss=0.069035 T/acc=0.588119 D/loss=0.185110 D/acc=0.474165 lr=0.000257\n",
      "INFO:root:[201] T/loss=0.068883 T/acc=0.589819 D/loss=0.180972 D/acc=0.484092 lr=0.000256\n",
      "INFO:root:[202] T/loss=0.069104 T/acc=0.590068 D/loss=0.186053 D/acc=0.478897 lr=0.000254\n",
      "INFO:root:[203] T/loss=0.068564 T/acc=0.591785 D/loss=0.180657 D/acc=0.487412 lr=0.000253\n",
      "INFO:root:[204] T/loss=0.068879 T/acc=0.590724 D/loss=0.184073 D/acc=0.483447 lr=0.000252\n",
      "INFO:root:[205] T/loss=0.069012 T/acc=0.593710 D/loss=0.194208 D/acc=0.475517 lr=0.000251\n",
      "INFO:root:[206] T/loss=0.068259 T/acc=0.589302 D/loss=0.181136 D/acc=0.476552 lr=0.000249\n",
      "INFO:root:[207] T/loss=0.068184 T/acc=0.590589 D/loss=0.192644 D/acc=0.478454 lr=0.000248\n",
      "INFO:root:[208] T/loss=0.068315 T/acc=0.590418 D/loss=0.186203 D/acc=0.472474 lr=0.000247\n",
      "INFO:root:[209] T/loss=0.067839 T/acc=0.588591 D/loss=0.183177 D/acc=0.470759 lr=0.000246\n",
      "INFO:root:[210] T/loss=0.068912 T/acc=0.589504 D/loss=0.184322 D/acc=0.487581 lr=0.000244\n",
      "INFO:root:[211] T/loss=0.068296 T/acc=0.589618 D/loss=0.189274 D/acc=0.477854 lr=0.000243\n",
      "INFO:root:[212] T/loss=0.067330 T/acc=0.594306 D/loss=0.177314 D/acc=0.474688 lr=0.000242\n",
      "INFO:root:[213] T/loss=0.067330 T/acc=0.588059 D/loss=0.185594 D/acc=0.485573 lr=0.000241\n",
      "INFO:root:[214] T/loss=0.067627 T/acc=0.590698 D/loss=0.190543 D/acc=0.481573 lr=0.000239\n",
      "INFO:root:[215] T/loss=0.067387 T/acc=0.592524 D/loss=0.191055 D/acc=0.480344 lr=0.000238\n",
      "INFO:root:[216] T/loss=0.067653 T/acc=0.587677 D/loss=0.181484 D/acc=0.487825 lr=0.000237\n",
      "INFO:root:[217] T/loss=0.067574 T/acc=0.590459 D/loss=0.193094 D/acc=0.473072 lr=0.000236\n",
      "INFO:root:[218] T/loss=0.067666 T/acc=0.593635 D/loss=0.188527 D/acc=0.481716 lr=0.000235\n",
      "INFO:root:[219] T/loss=0.067182 T/acc=0.590214 D/loss=0.188980 D/acc=0.473820 lr=0.000234\n",
      "INFO:root:[220] T/loss=0.067135 T/acc=0.589016 D/loss=0.195828 D/acc=0.464574 lr=0.000232\n",
      "INFO:root:[221] T/loss=0.066835 T/acc=0.591749 D/loss=0.184357 D/acc=0.487040 lr=0.000231\n",
      "INFO:root:[222] T/loss=0.067083 T/acc=0.588968 D/loss=0.184482 D/acc=0.478352 lr=0.000230\n",
      "INFO:root:[223] T/loss=0.066594 T/acc=0.591660 D/loss=0.192143 D/acc=0.474030 lr=0.000229\n",
      "INFO:root:[224] T/loss=0.067115 T/acc=0.587665 D/loss=0.188492 D/acc=0.483462 lr=0.000228\n",
      "INFO:root:[225] T/loss=0.066663 T/acc=0.594184 D/loss=0.195822 D/acc=0.471949 lr=0.000227\n",
      "INFO:root:[226] T/loss=0.066982 T/acc=0.588842 D/loss=0.185923 D/acc=0.483790 lr=0.000225\n",
      "INFO:root:[227] T/loss=0.066413 T/acc=0.594090 D/loss=0.190049 D/acc=0.464705 lr=0.000224\n",
      "INFO:root:[228] T/loss=0.067262 T/acc=0.594772 D/loss=0.185749 D/acc=0.481979 lr=0.000223\n",
      "INFO:root:[229] T/loss=0.066059 T/acc=0.588134 D/loss=0.188863 D/acc=0.481914 lr=0.000222\n",
      "INFO:root:[230] T/loss=0.066300 T/acc=0.592425 D/loss=0.185167 D/acc=0.480337 lr=0.000221\n",
      "INFO:root:[231] T/loss=0.066020 T/acc=0.596357 D/loss=0.191858 D/acc=0.480986 lr=0.000220\n",
      "INFO:root:[232] T/loss=0.066532 T/acc=0.590931 D/loss=0.186314 D/acc=0.476689 lr=0.000219\n",
      "INFO:root:[233] T/loss=0.065600 T/acc=0.593563 D/loss=0.194502 D/acc=0.474768 lr=0.000218\n",
      "INFO:root:[234] T/loss=0.066011 T/acc=0.592461 D/loss=0.185799 D/acc=0.471656 lr=0.000217\n",
      "INFO:root:[235] T/loss=0.065895 T/acc=0.593993 D/loss=0.197220 D/acc=0.485956 lr=0.000216\n",
      "INFO:root:[236] T/loss=0.066068 T/acc=0.592324 D/loss=0.184639 D/acc=0.471202 lr=0.000214\n",
      "INFO:root:[237] T/loss=0.065183 T/acc=0.597558 D/loss=0.186431 D/acc=0.478200 lr=0.000213\n",
      "INFO:root:[238] T/loss=0.066219 T/acc=0.598004 D/loss=0.189573 D/acc=0.487008 lr=0.000212\n",
      "INFO:root:[239] T/loss=0.064877 T/acc=0.593153 D/loss=0.192827 D/acc=0.475393 lr=0.000211\n",
      "INFO:root:[240] T/loss=0.065673 T/acc=0.595736 D/loss=0.187904 D/acc=0.469277 lr=0.000210\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import chainer\n",
    "\n",
    "#from listnet import dataset, training\n",
    "#from listnet.model import ListNet\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def run(args):\n",
    "    logging.info(\"Loading dataset\")\n",
    "\n",
    "    trains = create_dataset(args.train)\n",
    "    logging.info(\"loaded {} sets for training\".format(len(trains)))\n",
    "\n",
    "    dev = create_dataset(args.dev)\n",
    "    logging.info(\"loaded {} sets for dev\".format(len(dev)))\n",
    "\n",
    "    test = create_dataset(args.test)\n",
    "    logging.info(\"loaded {} sets for test\".format(len(test)))\n",
    "\n",
    "    listnet = ListNet(trains[0][0].shape[1], 200, 0.0)\n",
    "    optimizer = chainer.optimizers.Adam(alpha=0.0007)\n",
    "    optimizer.setup(listnet)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))\n",
    "    #optimizer.add_hook(chainer.optimizer.GradientClipping(5.))\n",
    "\n",
    "    train_itr = chainer.iterators.SerialIterator(trains, batch_size=1)\n",
    "    train(listnet, optimizer, train_itr, 1000, dev=dev,\n",
    "                   device=None)\n",
    "    loss, acc = forward_pred(listnet, test, device=None)\n",
    "    logging.info(\"Test => loss={:0.6f} acc={:0.6f}\".format(loss, acc))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--train', required=True, type=str,\n",
    "                   help='SNLI train json file path')\n",
    "    p.add_argument('--dev', required=True, type=str,\n",
    "                   help='SNLI dev json file path')\n",
    "    p.add_argument('--test', required=True, type=str,\n",
    "                   help='SNLI test json file path')\n",
    "\n",
    "    # optional\n",
    "    p.add_argument('-g', '--gpu', type=int, default=None, help=\"GPU number\")\n",
    "    args = p.parse_args([\"--train\", \"MQ2008/Fold1/train.txt\",\n",
    "                         \"--dev\", \"MQ2008/Fold1/vali.txt\",\n",
    "                         \"--test\", \"MQ2008/Fold1/test.txt\"])\n",
    "\n",
    "    run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
